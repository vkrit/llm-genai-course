{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n", "from peft import get_peft_model, LoraConfig, TaskType\n", "from datasets import load_dataset\n", "import torch\n", "\n", "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n", "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train[:1000]\")\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(model_id)\n", "def tokenize_fn(example):\n", "    return tokenizer(\"### Instruction:\\n\" + example[\"quote\"] + \"\\n### Response:\\n\", truncation=True)\n", "tokenized_ds = dataset.map(tokenize_fn, batched=True)\n", "\n", "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n", "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)\n", "model = get_peft_model(model, peft_config)\n", "\n", "args = TrainingArguments(output_dir=\"./results\", per_device_train_batch_size=1, gradient_accumulation_steps=4, logging_steps=10,\n", "                         learning_rate=2e-4, num_train_epochs=1, fp16=True, save_total_limit=1)\n", "trainer = Trainer(model=model, args=args, train_dataset=tokenized_ds)\n", "trainer.train()"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}