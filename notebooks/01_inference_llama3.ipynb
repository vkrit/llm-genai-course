{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n", "import torch\n", "\n", "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n", "tokenizer = AutoTokenizer.from_pretrained(model_id)\n", "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n", "\n", "prompt = \"Explain how transformers work in NLP.\"\n", "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n", "outputs = model.generate(**inputs, max_new_tokens=100)\n", "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}